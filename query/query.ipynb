{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "\n",
    "urls = [\n",
    "    \"https://www.youtube.com/watch?v=HAn9vnJy6S4\",\n",
    "    \"https://www.youtube.com/watch?v=dA1cHGACXCo\",\n",
    "    \"https://www.youtube.com/watch?v=ZcEMLz27sL4\"\n",
    "]\n",
    "docs = []\n",
    "for url in urls:\n",
    "    docs.extend(YoutubeLoader.from_youtube_url(url, add_video_info=True).load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Add some additional metadata: what year the video was published\n",
    "for doc in docs:\n",
    "    doc.metadata[\"publish_year\"] = int(\n",
    "        datetime.datetime.strptime(\n",
    "            doc.metadata[\"publish_date\"], \"%Y-%m-%d %H:%M:%S\"\n",
    "        ).strftime(\"%Y\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'HAn9vnJy6S4', 'title': 'OpenGPTs', 'description': 'Unknown', 'view_count': 9190, 'thumbnail_url': 'https://i.ytimg.com/vi/HAn9vnJy6S4/hq720.jpg', 'publish_date': '2024-01-31 00:00:00', 'length': 1530, 'author': 'LangChain', 'publish_year': 2024}, page_content=\"hello today I want to talk about open gpts open gpts is a project that we built here at linkchain uh that replicates the GPT store in a few ways so it creates uh end user-facing friendly interface to create different Bots and these Bots can have access to different tools and they can uh be given files to retrieve things over and basically it's a way to create a variety of bots and expose the configuration of these Bots to end users it's all open source um it can be used with open AI it can be used with other models as as we'll see um and it's an exciting way to create a a GPT store like experience if you're building a more focused platform an internal platform or any of that so we launched this a few months ago actually right when uh open AI released their GPT store and but we haven't really dove into what's going on or how to use it um and so there's several things that I want to cover in this video there's maybe two main areas one I want to talk about it as an enduser facing application so how can you interact with we we have a we have a simple research preview hosted version of this um how can you interact what can you do what the functionality in the second half of the video I want to talk about how openg gpts is built um we do not intend to uh you know monetize openg gpts we want this to be a platform that people can clone and spin up their own versions of the GPD store or an internal platform or things like that so the second half of this video will focus more on how to build this platform and some of the considerations that we made please note that I am filming this on January 30th we are going to continue working on this continue changing it so if you are watching it later the hosted version or the code may have changed so starting with how to use it um the the the first thing that you'll want to do is create a bot and we actually have three different types of bots that one can create I'm going to start with assistant which is the default bot and and is also the most powerful one so the assistant can use an arbitrary number of tools and you can give it arbitrary instructions the llm will then take those instructions and those tools and decide which tools if any to call um get back the response and then and then continue on its way so let's create one that has access to the internet um so let's name it internet bot um let's give it some instructions you are a helpful weatherman always tell a joke when giving the weather let's scroll down there's a bunch of tools that we can use I'm going to use the search by tavil um they're a LM focused search engine um and then the currently the only agent type that we have supported in the research preview is with GPT 3.5 turbo but we will show in the code when you're building this you can enable CLA you can enable uh Google Gemini you can use gp4 you can use Azure you can use Bedrock but this is just the one that we have in the research preview so I'm going to save uh this now I'm going to try chatting with it let's say hi so several things to notice um one it's streaming responses so we put a lot of work into streaming of tokens this is important for uh uh you know most chat-based applications um it's got some feedback as well so I can give it thumbs up thumbs down um and start recording feedback and and when we talk about the platform side of things we'll see how that can be used now let's ask it a question that will require the search ability what is the weather in SF currently we can see that it decides to use a tool and so importantly it lets us know that it's deciding to use a tool and then it also lets us know what the result of the tool is and then it starts streaming back the response so this is streaming not just tokens but also these intermediate steps which provide really good visibility into what is going on we can see here we can see the response that we got back from tavil um and then we can see um the response from the AI and so there's lots of dad jokes in here this is using open aai tool calling under the hood so we can also ask it to look up multiple things and see multiple tool calls in parallel what about in LA and in New York so we can see that it now calls two things two things it C it call calls the tavil search Json for weather in Los Angeles the tavil search for weather in New York we can look at the responses this is for Los Angeles this is for New York um and then we can uh see the response here so this is an example of parallel tool calling that's enabled with open ai's most recent feature their their tool calling so this is the assistant it's using Tools in an arbitrary way to accomplish its task let's uh go create a new bot and now let's create one uh of type of rag so rag is really focused on retrieval over arbitrary files that you can upload so you can upload files and then the uh you can also give it custom instructions um and and then the bot will respond based on those files what is the difference between this and the assistant because in the assistant you could also upload files and you could choose retrieval as one of many to tools the main difference is that this is much more focused on answering questions specifically about files that you upload so this means that it will always look things up in a retriever it's actually hardcoded and we'll show this in in the um when we talk about the the back end but it's actually hardcoded that it will always do a retrieval step here the assistant decides whether to do a retrieval step or not sometimes this is good sometimes this is bad sometimes it you don't need to do a retrieval step when I said hi it didn't need to call it tool um but other times you know the the llm might mess up and not realize that it needs to do a retrieval step and so the rag bot will always do a retrieval step so it's more focused there because this is also a simpler architecture so it's always doing a retrieval Step at the start and then it's actually always responding after that it's not doing potentially two retrieval steps it's not doing an iterative search this is a very simple rag architecture um which has its downsides it's not as flexible it can't handle multihop questions things like that but it's much more focused streamlined and that means it can work with simpler models as well so we have actually enabled mraw um through fireworks to work on uh this type of gbt a rag gbt so I have a uh I have a PDF here that I'm going to upload um this is Spade this is a paper Shrea recently wrote um at Berkeley and it goes over setting up kind of like a testing pipeline um Bas for for your prompts super interesting paper um I'd uh I'd recommend reading it regardless um let's upload the PDF um we can we can change the message slightly um let's still use 235 turbo research Spade let's save this now it's taking a little bit longer to save because what's going on under the hood is that it's injesting the file now it's in a method where it can be retrieves and and I'll talk about this when we talk about the back end as well um if let's take a look at this paper um and let's figure out something we can um ask what is a propped Delta so here it always uses retrieval the the it calls the retrieval function it gets back documents we format documents nicely um and you can see what these documents are and then it responds here so this is an example of a simple rag bot which always does retrieval hyperfocused on rag if you want to ground a bot in some external data source that you can upload this is probably the simplest and most reliable way to do that again it's a little bit less trustworthy than uh than than the chat bot we have it as a separate type of Bot because it is simpler so that means that it can work with other models like mraw which is an open source model so it just provides more flexibility and that flexibility is the same reason we have a third type of Bot this chat bot this is just solely parameterized by the instructions so you can write out long complicated instructions for how it should behave you can give it a character and it can act like that again because this is simpler it can work with simpler models let's create an example chatbot we'll create one that responds like a pirate so you are a helpful pirate always respond in a pirate tone pirate save this hi and we get back a response and pirate so a lot of gpts in the GPT store are really just complicated system prompts and so for those you can create them using using this chatbot type the other a lot of the other gbts that I've seen at least are the rag style chat Bots where they're parameterized by a system prompt and then also um and then also a bunch of uh files that you can upload to give it information besides what it knows about and it can search over those and so these you know these are much simpler architectures than the assistant but for a majority of use cases they're actually completely fine the nice thing about assistance is you can do more complicated with things things with it and you can also equip it with arbitrary tools and so here there's a bunch of tools that we've enabled in the back end by default but you can easily add your own and and explore with those um and so that's part of the power of this platform being open source as well you can Fork it you can make it your own you can deploy it either to end users internal company users anything like that other things that I want to highlight in the front end um you can make Bots public this means that you can share links um you can see old conversations and jump back in um you can create new chats um when you're in a conversation you can click in here and see the bot that it is using um when you create a new chat you can look at the saved Bots that you have if you have any public Bots they'll be down here as well and yeah that's basically it for an overview of the front end I'm now going to switch to talk about some of the architecture of the back end which will be really helpful if you want to Fork this and make it your own so this is the openg gpts repo it's under the linkchain org um there's some instructions here there's a good read me on everything that's in here there's some Docker composed files for deploying it um there's some other uh uh files for environment variables the API docs things like that for this I really want to focus on the back end so we can take a look at what's going going on in here um and most of the logic here there's there's some requirements files most of the logic here is going to be in app and so we can see there's a bunch of different files here so there's a few things that I want to draw attention to first let's uh let's maybe look at agent types so when we talk about the assistant in the in the uh in the front end this is where these agent types are defined and so different assistants have different architectures that are going on behind the hood let's take a look at the open AI agent for example so important to note this is built on top of L graph so if you aren't familiar with L graph you should definitely go check it out it's a really easy way to build these types of cyclical agentic Frameworks so we have this open AI agent executor which takes in a list of tools an llm a system message and then a checkpoint um and we'll see how we use this so first um we're going to create basically this uh quote unquote agent and this agent is responsible for taking in messages and deciding what to do next so there's first a step where we format um the messages um and so we add a system message um and that's defined up here and then we pass in the rest of the messages the llm also then has access to the tools so we bind it with tools and then we combine it using this pipe syntax to get this agent we next Define a tool executor this is a class that is just does some minor boiler plate for calling tools um and then we start to define the different nodes of the graph so first we Define the function that determines whether to continue or not this is should continue it looks at the messages if there's no tool calls then it finishes if there is tool calls then it will continue and we'll see how we'll use this later on We Now define the node that calls the tools um so here uh we uh take in the list of messages if we we get the last messages we get the last message we know that it involves a function call um because otherwise we would have ended um we get all the tool calls so when there's multiple tool calls we get them all we then pass them in uh into here into the tool executor in a batch so it runs them in parallel um and then we append them to the the messages and we return the messages from this node so importantly this will use a message graph and so this means that every node in the graph should return a message or a list of message so here we return a list of messages the other node that we add the agent this is just this is basically an llm call the LM call returns a message so both of the nodes return messages we set the entry point to the agent so when anything comes in we go to the agent we then add a conditional Edge so after the agent is called we then check this should continue function if uh it says continue then we call the action node otherwise we finish um we add an edge so after we call the tools in the action node we go back to the agent we then compile it compiling we're passing in check pointer equals checkpoint this is basically a way to persist um the state of the graph so we're persisting all the messages that that happen so this is nice for a few reasons um the main immediate way that we are using it right now is we are saving it to reddis and then we're showing that in the front end so in the front end when you see that we save the chat history um those are pulling from reddis the way that is getting saved to redus we don't have separate functions saving everything we just pass in this checkpoint and it all kind of gets written there there's similar things for the Google agent Google agent looks very similar there's some minor differences uh because it uh uh is a a Google agent so it's a little bit different it doesn't have tool calling it has function calling there's also an XML agent designed to work with anthropic models and so this is different as well so it uses some of the prompts um and things like that so those are the agent types um we also have a uh really simple executor for the chatbot chatbot just calls the message once with the system message so it has a really simple node the chatbot node just calls it and then ends dead simple um but we use the message graph again uh so that it it all of these Bots can speak kind of like on the same kind of like State um which will make it nice if we want to do any multi-agent or multibot things in the future um and then we also have this retrieval bot so this retrieval bot um basically it it's it had it's simpler than the agent node um so it doesn't have any Loops but it's more complex than the chatbot node so we have this prompt template um this is used for coming up with a search query to pass to the retrieval um and so we can see that we have the conversation here and then we generate the search query um and then the response pop template takes in instructions and then has context so what's going on here is that we have this get messages function um and basically what's going to happen is we're passing all the state around as messages um and so part of that um has the the search query involved and so we can see here if we scroll down I'm going to scroll down to this graph we F we have this invoke retrieval uh node we have this retrieve node and then we have this response node and then we always invoke retrieval at the start and then we go from invoke retrieval to retrieve and then we go from retrieve to response and then we end so remember how I said the difference one of the differences with the retrieval with the rag bot was that it always did retrieval this is this is what's happening so the invoke retrieval node it's always going to return an AI message that calls retrieval so we're not even actually calling the language model sometimes so if the length of the messages is one this means if it's the first message in the conversation we're just going to look up whatever that first thing was um so this is a little cheat that a lot of rag based systems or conversational rag Bas systems do is the first time someone types in something in we just look up that input the issue start starts to happen when you have a conversation so if I have a follow-up question or you know a series of follow-up questions I don't really just want to pass that follow-up question in because it could be referencing things previously and so what I do instead is I call um this other method um which is itself a call to a language model um and so this is using the search prompt to generate a search query um and then I specify that as the the retrieval thing then the retrieval thing is just calling the retrieval it's passing the results in this function message um and then the response is just a call to the language model with some formatting into this prompt so if we see this get messages thing what we're doing is we're getting the uh most recent message which is the result of calling this tool we know that it will always be that because we have this this determined graph we're getting the response from that and we're we're formatting that into the system message um and yeah so basically Al we're constructing the chat history the chat history is going to be AI messages that do not have function calls if they do have function calls then they are the result of retrieval steps and we don't want to include those in the messages that we pass to the final LM um and then uh the chat history also includes human messages um and then it also includes the system message system message is where we use the system message prompt as well as the context that we retrieve from the documents so that is the so those are the three types of um Bots that we have they're all put together in this agent file and this is where we also start to use um uh uh configuration basically so configurable fields and configurable alternatives are something that exists in linkchain and they're really handy when you want end users to be able to configure things or sometimes when you want to do the configuration on the Fly for example if you want to randomly select a model to use and you want to configure that on runtime and basically the way that that looks like and the way that we've implemented it here um is that we have this idea of like a configurable agent that wraps around a binding and there's these different parameters on here inside the initialization we take in the parameters and we construct the agent um and then we pass it in and then what we'll do down below and I'll return to this later on what we do about down below is we initialize this configurable agent and then we Mark certain Fields as configurable so agent field here is configurable with an agent type um the system message field is configurable with a system message um the assistant ID this is the assistant ID of the um of the bot that you've selected the tools are configurable the retrieval descriptions configurable and so a lot of the uh fields that we create the bot with are configurable and those are exactly what we expose in the front end of uh open gpts we've also exposed some configurable Alternatives and these are the different architectures so uh there's um there's a chatbot which follows the chatbot architecture and there's the chat retrieval which follows the rag bot architecture and if we scroll up we can see that chat retrieval uses this idea of configurable retrieval um so this is the same kind of like runable binding it's got these same parameters we Mark these fields as configurable um and then chatbots exactly the same and so basically the difference between the fields these are things that go in again we have like three seate types of high Lev Bots the configurable fields are things to configure this specific the the assistant type of Bot and then the alternatives are completely different kind of like Alternatives that you could even use so there's two different ways that you can configure things fields and Alternatives one more thing I want to highlight is just the ingestion bit um so there's the ingest pipeline um which is really quite simple this is something we're going to look to expand on in the future um and so if if you want to help make this uh retrieval more advanced would love that but basically we just split documents um and then we add them to a simple Vector store um and then the retrieval um is here part of tools I believe um and we can see here that it's just a uh retriever um really simple um really simple and then we just filter based on the name space so uh based on the assistant that you're using the assistant only has access to the files that were uploaded to it um so there's a lot that we can do to improve this we can add a reranking step um there's already some sort of query transformation going on based on the Bots um but we can add more things um and so improving the retrieval is one aspect that we want to lean into in the future um the last thing I want to point out is that this all integrates with Lang Smith so if you're a little bit lost about what the different types of agents are what exactly is happening uh uh what got configured um that's totally normal these applications start to get really really complex and that's where lsmith comes in handy so the deployed version we have hooked up to a project in lsmith and so we can click on here we have this open gpts example project um and so if I go in here and I click on a trace I can see exactly what's kind of like going on under the hood um and so here um this is if you remember this is actually the system message that I added um when I configured the pirate chatbot um and so this is a system message this is what I said and is the response and so I can track it and so I can also leave when I when I left the thumbs up and thumbs down um I can track that here um and so I believe yeah so here I left the thumbs up on this was this was the weatherman um I I got back this response if we click in what's here I think yeah so this is when we gave it access to the Search tool um and so you can see like exactly what's going on um this is a pretty simple agent because it just responded um but I can see the feedback here as well it's at the top level so I can see the feedback here as well I have a user score of one um and so yeah Lang Smith is a whole separate concept but I just want to point out that openg gpts if you deploy it it's integrated with Lang Smith um if you need Lang Smith access shoot me a DM on Twitter or LinkedIn and can get you access to that that's pretty much it for what I wanted to cover um hopefully this gives you a good sense of both how to use the front end the the research example as well as how to configure the back end um so I think I think the important thing to note that I would highlight is that you can put any different type of architecture behind here right now all the architectures the three different architectures we have the assistant architecture the rag architecture and the chatbot architecture they all use this message graph implementation which passes around a list of messages and I really like this because it's a single common uh kind of like interface that'll make it easy to add on different bots in the future um so there's a lot of things we want to do in the future one of those is having like multiple bots on the same thread or allowing you to switch Bots between threads um and so having this common state representation will make it easy to do that that's pretty much all I got hope you guys enjoyed this we're really excited about open gpts if you want to use this I mean one feel free to Fork it but also feel free to reach out to us we're more than happy to help thanks\"),\n",
       " Document(metadata={'source': 'dA1cHGACXCo', 'title': 'Building a web RAG chatbot: using LangChain, Exa (prev. Metaphor), LangSmith, and Hosted Langserve', 'description': 'Unknown', 'view_count': 7988, 'thumbnail_url': 'https://i.ytimg.com/vi/dA1cHGACXCo/hq720.jpg?sqp=-oaymwEmCIAKENAF8quKqQMa8AEB-AHUCIAC0AWKAgwIABABGEQgWyhyMA8=&rs=AOn4CLDz7KQK8VbZMWIgIheOS8TQzo9sNw', 'publish_date': '2024-01-26 00:00:00', 'length': 1950, 'author': 'LangChain', 'publish_year': 2024}, page_content=\"hey folks I'm Eric from Lang chain and today we're going to be building a search enabled chatbot with EXA which launched today um let's get started uh to begin let's go through the main pieces of software that we're going to be using um to start we're going to be using Lang chain um Lang chain is a framework for developing llm powered applications um it's the company I work at and it's the framework we'll be using we'll be using the python library but we also offer JavaScript library um for folks building in that language um the second thing we're going to be using is EXA EXA is a llm focus search engine which allows us to retrieve results uh from the web to provide additional context for our uh generation chatbot um before giving you your results uh Lang Smith is what we're going to be using for debugging and observability today it is the first party um observability tool built by the us at Lang chain um we're going to be using it to inspect some traces and we're also going to be using um the hosted Lang serve product which is part of Lang Smith um in order to host our application in the end um and uh we're also going to be using Lang serve kind of as previously mentioned uh the open it is an open source package um that allows you to host your chains as rest endpoints um and uh we're going to be using the hosted one as well within Lang Smith uh let's dive it so first we're going to be building our chain in a jupyter notebook then we're going to be porting that over to Lang serve as a rest endpoint we're going to be uh playing with that in the playground that's provided there and last but not least we're going to be hosting that in Lang serve in order to um get it accessible from anyone on the internet um so let's dive in so for our notebook um I'm just going to be using the Jupiter notebook in my vs code instance um first we're going to want to install some of our dependencies um as some of you may have noticed we're starting to split out the package into um multiple smaller partner packages as well so today we're going to be um pip installing uh Lang chain core which offers um we're going to be mostly using the runable utilities as well as prompts from there um we're going to use Lang chain open AI um in order to um use the GPT 3.5 model for Our Generation you can obviously install any uh other llm into this application as well um and last but not least we're going to be installing the brand new L chain exop package um which is going to give us our retriever which allows us to kind of search uh the web the Lang chain EXA package also offers some tools which enable agents to um have search capabilities as well um but for today's generation use case we're going to be using uh the retriever um once you have that installed um we are going to set some environment variables um so to do that we'll have OS and we'll set um our open AI API key and our X API key um um I already have these set in my environment so I'm not going to fill this out for yall today um but you can provision your own open AI API key at uh platform. open.com and uh I'll link the exod docs um on how to get your own xit API key with some free um search credits um on it as well to this video um the other thing that we're going to do is we're going to set some environment variables that um enable us to use Langs Smith um Langs Smith is in private beta at the moment feel free to DM me in order to get access to it um so in order to do that um we are going to set the Lang chain facing uh V2 uh to do that we're going to set the L chain tracing V2 environment variable we're going to set our um L change API key and we're going to set our Lang chain project as well um the last step is optional um and it just allows us to uh find um the traces for what we're doing today a little bit easier um and most importantly we're going to build our chain um so the rough architecture of today's chain is we're going to first uh take some sort of user query um we're going to search EXA to get some documents related to that um EXA has this great feature we're going to use called highlights which kind of summarizes the highlights from any of the sources it retrieves um and so we're going to be able to PL those into an llm uh we're going to kind of wrestle with um formatting that prompt in a way that the llm understands uh well for a little bit and then we're going to pass that into the llm for Generation Um so to start uh let's actually just play with Exel a little bit and see what that looks like um so we can import our retriever uh from the L chain EXA package we're going to be using the retriever today um let's just use the default settings and see what documents we get from that so we can just run retriever do invoke of uh best time to visit Japan um and if we inspect those um or inspect the first one uh we can see that we have some content which is actually going to be the entire page content um of that website so it's going to be rather large um and we'll also have some metadata on that document um and so here we'll have um some like kind of page titles URLs um as well as um if we pass the highlights feature in we'll actually get some highlights as well so let's pass in um highlights as true and then let's actually only get the three most relevant results just so we can save some of our EXA credits um so here we'll now see that we get um this highlights array which is going to summarize kind of the main points um of this page um so we're mostly going to be using that uh highlights field as well as um the URL uh since we want our generation bot to to site its sources in the generations it do does um so with that let's try to create some sort of um retrieval chain which is going to format all of those outputs in a way that we want to pass it to the Ln um so the kind of first step of that is we're going to run our retriever um and just to keep it clear we can Define our retriever down here as well um keeping the kind of final output in a single cell makes it little easier uh for us to convert this to a l serve application at the end um we'll also want to format this with um a prompt template so let's import that as well from Lang chain core. prompt and uh we can import other things as we go on so first we're going to start with our retriever um then we're going to want to uh format that list of documents in some sort of way so let's pass that into some sort of documents chain um or single document chain uh and we'll have it operate on each element of that list so we can kind of call map on that um and let's define that up here and so the Assumption here is that input is going to be that single document um here we'll have this be a runnable Lambda where we'll take uh that document and we will pass back the highlights as well as the uh URL of that document so here we'll have document. metadata highlights and document metadata URL um and we'll need to import um our rable Lambda as well which is going to be from L chain core runnables P runable Lambda um and then the next step is we're going to want to format this with some sort of prompt so let's pass this into um our document prompt which we can Define up here as prompt template uh from template and here uh let's actually just wrap the whole thing in XML tags just because it'll allow the llm to kind of isolate information from um the inputed documents a little bit better um and then we can include the URL and the highlights um and in this format the highlights are going to get past in as a list of strings that's probably okay um if we wanted to make this um a little more intense and we can do this later we can actually uh split those out into different sections um but because all the highlights come from the same URL uh this is probably okay for the uh point of our application today um so that kind of forms our document chain um where at the end of this we're going to have a list of um prompt values um which we can use Downstream um and then let's then combine all of those kind of source information contexts um with another Lambda um which is going to take all the docs and we're going to want to join some sort of list of these so we can do the text of each of those prompt values um for each of those documents so let's see what we actually get from that um it looks like we are missing a comma somewhere um because a string object is not callable it looks like here I should have called backend. join at this um and we can see that we're getting kind of a single string back with our three sources uh as well as the highlights of uh each of these sources uh where it's recommending public holidays uh the most romantic time of the year um and the rainy season um which is quite interesting um but now we kind of have our retrieval chain which we can use in our broader chain down here um for further processing so um our overall chain is actually going to take a very similar format to a general rag chain um where we're going to have some sort of um runnable parallel to start we're going to want to um start with um plumbing through the query which is just going to be a runnable pass through um this is going to just pass the users's query onto the next step through this query uh key and then we're going to want to pass some context Tex um which will be our retrieval chain then we'll want to pass that into some sort of generation prompt which is going to take the query and context um and format it for the llm and then we'll pass that into our llm um so let's fill this out a little bit so first We'll add some imports um we'll import our runnable pass through and our runnable parallel from Leng chain core um we'll Define a generation prompt as uh and let's do this one as a chat prompt so we can use an a chat model for the llm um so we'll use a chat prompt template for messages um and we'll have some sort of system prompt as well as the content which we can fill out in a second um we'll need chat PR template for that for that uh which co-pilot is quite nicely supporting for us um and then we'll just use the default chat open AI uh which uses GPT 3.5 turbo um for our llm so we can import that from Lang chain open aai um import chat open AI um this is kind of the new format of importing from some of those partner packages um cool so let's fill out our prompt um so let's tell our llm that you are an expert uh research assistant um you use um XML formatted contexts to um to research people's questions uh and then we'll actually format this uh query that we're going to ask down here where we're going to say please answer the following query based on the provided context and we'll pass in our query um add a little separator and then continuing with some of the XML tags we can pass in our context here which is just going to be this some form of the string up here which is going to have multiple sources um let's also add an instruction to please site your sources at the end of your response um cool so now we have our chain defined let's see if we're able to invoke it and we'll continue with our same question and it looks like it's giving us a nice and long response um detailing some things and hopefully at the end it'll have some sources of what it used to generate that um so now that we have that constructed and tested um let's actually go back to Lang Smith and see what it generated for us um so here in Lang Smith we can see um the default view is going to show us the most uh kind of relevant pieces of the trace so in this case because it's a retrieval augmented generation bot um it's going to be our retrieval step such that we can inspect which documents it's actually retrieving here um as well as the llm call so we can see uh what prompt we're actually passing in so we can see that we're formatting our contexts and sources um properly early here which is good as well as the output from the llm at the end um if you want to inspect kind of what's happening in each of those runnable Lambda steps uh you can inspect further and see um that the inputs to each of those kind of Lambda steps is going to be one of those documents and then we're outputting um like the highlights DL segment and then formatting that with our prompt template uh if you recall from when we were constructing it um so now we have kind of our uh fully constructed chain uh for our uh search enable chatbot with XF um and now let's convert that to Lang serve um so to do that we'll go back to vs code um and here we're going to um start with uh installing the um Lang chain CLI um we'll just do this in a terminal down here um you can just install um recommend running it with upgrade in case you already haveen it installed from uh either using Lang chain templates or something else um you can just run install upgrade Lang chain clly um then you can uh run the line chain commands so first we're going to initialize our app with Lang chain app new um I'm just going to create it in the current perplexity directory um if you give it a name it'll just create a directory uh within that um we will skip adding packages just because we're going to paste in a chain that we have defined up here um the packages are just L chain templates um and then we can see that it has bootstrapped an application for us um with a Docker file P project toml a readme um as well as an app directory and a packages directory um we're actually not going to be using the packages directory um as mentioned that's for L chain templates and we're just going to be playing in this uh server.py file in the app um so first we're going to um add some dependencies um I use poetry for my dependency management um it allows um it to automatically update this P project toml file which is quite nice um we're going to install the same set of dependencies um that we had in our uh jupyter notebook so we're going to poetry add um linkchain core um and we'll see that show up in the Pi Project tomel and then we're also going to add L chain open Ai and Lang chain XF um I'll add those and then be back in a second and now that we have our dependencies installed um also added the command to the notion doc over here just so it's easier to follow along um now we will copy our chain over into our application um I like keeping the server.py file a little bit cleaner in order to just keep the um fast API kind of app logic separate from the Lang chain chains um so let's actually just Define a chain. piy file next to this um where we can um paste in our um chain from this other document um so I'm pretty sure I can just paste in these two um cells and we'll remove the invoke in the middle um so first we'll Define a retrieval chain and then down here we will uh Define our regular chain um and we can even put all of our Imports together up here to keep uh vs code a little bit happier um and we'll save that file and now our chain should be accessible from our server file um and then by default it doesn't have any rout defined but it gives you this nice little stub and so we'll actually just add uh that chain which we'll import um from app. chain import chain um because we have it in chain. piy as well is chain is the name of our variable um and then that will just host uh this endpoint at the kind of default SL endpoint um if we want we can define a path um as kind of Slash search or something like that um which could be kind of nice so we've copied the chain over we've added the routes um and now uh we can serve it um so let's poetry run L chain serve um and see if that works hopefully I remembered to add all the dependencies um and if you get this message you're probably in business um so from here uh we can see that we're hting it on Port 8000 so we can click onto that um by default it'll show us the stocks page um but we want to go to the search playground um right now it's telling me that there is no applicable renderer found which is too bad um let's go back to our chain and see if we can add a uh type definition for the input um where here we have our input type uh set just to a string um and see if that does anything for us and perfect so now we can see that we get our input as a string um generally Lang serve will try to detect uh the defined input type for our input over here I think it got a little bit confused because the input of our runnable pass through and our retrieval chain um could be something other than a string um or some sort of formatted dictionary and so uh we had to Define our input type as a string if you want to pass in uh multiple um pieces of the input you are welcome to Define this as like a pedantic base model um which we can play with in a little bit if we want um so here let's try um when is the best time to visit Japan um and we can see uh in the link of playground it actually will show us kind of the individual steps that it's running as part of this um so right now it's kind of running through the retrieval step before passing it all through to the llm um it's taking a little bit of time so we can check our stack Trace right now it is telling me that I am using a wrong API key um so let's try adding my API keys to my environment again and running Lang chain serve again um and then we can try this again and that looks to be going better so we can see that it ran the intermediate steps for retrieval um and then in the playground we actually get streaming output as well um which is pretty good uh so here we get kind of a guide for uh why it might depend on several factors let's ask specifically for a ski trip um and see if it gives us a more concrete answer and here it does and it starts planning our trip as well figuring out which ski area we're going to go to um the other nice thing about Lan serve is even when using it hosted uh locally like this um we will actually see these traces show up in lsmith as well um so here because the playground is using streaming output the only difference is the output is going to be um kind of uh some sort of chunked output but Lang Smith handles that pretty well um in terms of displaying that to you uh and overall our Trace actually looks pretty similar um where we can see kind of the that's actually formatted in the um chat open AI step and we can even see which documents are actually plumed through um one note is this app that we build is just uh using the highlights that are given to us by EXA um if you want you can also operate on the entire page content um but the EXA highlights work pretty well and they save you a lot of tokens um as mentioned earlier um if you want access to Lang Smith uh feel free to DM me um and I can get you access to that okay and then last but not least uh we're going to um host our chain in hosted Lang serve um so hosted Lang serve is this deployments tab in Lang Smith um right now I don't have any deployments in my um uh in this account um so we can go over to uh new deployment and see kind of what steps it wants us to do we'll name our deployment so here we'll have this as perplexa T um emphasis on EXA and then we'll want to import our repository from GitHub um so in this case I'll actually just deploy um this uh project to just a public repository um you can also deploy uh private repos and that all works well too um so here we'll want to initialize our repo um we'll see what it is going to try to commit um just in case we've created any of these py cach folders let's actually create a g ignore where we'll oh we already have py cach in there so let's try adding everything and see what files it's going to add that looks pretty good um and uh we can commit um that project there um I'm realizing I'm using a a local development version of the EXA package because it's not published yet um so I'm going to fix that and be back in a second okay I fixed that dependency um that will not be a problem for you because you'll be using piie dependencies if you do ever use um kind of local development dependencies in your your application you will have to either make those git dependencies or uh publish them to piie in order for hosted L serve to be able to download them um unless it's kind of within the same directory um so let's uh commit our project um and then we can create a repo um we'll push our existing local deposit repository we'll call it FX T um lank chain perplexity uh we'll push it to just my personal ER don't need a description and we'll make it public um we can add the remote As origin and we will push our local commits to it um cool so now we can and go over to GitHub and see if that is um properly showing up as L chain perplexity um and looks like we are in business um so if we go back to our deployments we can import that repository from GitHub um we'll give it access to Lang chain perplexity and we will authorize and we can close that back here we can see that we have access to that repository if you give access to all your repos um you'll just be able to select that here um it's stored in the root of the directory we're using the main branch um and we'll need to set some environment variables so here we're going to want our open AI API key we're going to want uh that to be a secret and we're going to want our EXA API key as well um I'm going to populate those two and be back in a second okay now that we have those environment variables Set uh we can submit our deployment and uh hosted Lang serve will kind of take over from here um it'll provision a machine for us um hosted Lang serve is very well integrated with Lang Smith um such that all of our traces will actually populate in its own project over there um and we'll be ble to see all the traffic that comes into it um through the hosted endpoints here um for hosted Lang serve um this is also in private beta um and if you would like access to it uh feel free to DM me about that as well um so we set our environment variables and um I'll kind of cut out the section where this is deploying um and then we can test it in the hosted playground okay and now our uh posted Lang application is deployed we can see our deployment shows up up here um we can go to it we can see that uh the docs for it are showing up and we can even go to our same search playground endpoint um and see if we can ask a question um such as when is the best time to visit Japan for the cherry blossoms and we get a result in recap uh today we uh went through uh building kind of a search enabled chatbot with EXA um using Lang chain EXA Lang Smith and Lang serve uh we started by developing our chain in a jupyter notebook um that was kind of where the bulk of the Lang chain specific logic was um then we ported that over to a lang serve application using the Lang chain CLI um and kind of monitored what was uh coming out of that through lsmith traces and last but not least we deployed our Lang serve application in hosted Lang serve uh also a beta product um as part of Lang Smith um again if you want to get access to the private beta of Lang Smith or hosted Lang serve um please let me know um and and thanks for listening bye-bye\"),\n",
       " Document(metadata={'source': 'ZcEMLz27sL4', 'title': 'Streaming Events: Introducing a new `stream_events` method', 'description': 'Unknown', 'view_count': 2825, 'thumbnail_url': 'https://i.ytimg.com/vi/ZcEMLz27sL4/hq720.jpg?sqp=-oaymwEmCIAKENAF8quKqQMa8AEB-AHUCIAC0AWKAgwIABABGGQgZChkMA8=&rs=AOn4CLBZCZLx203dWs9DKzqrJY3Lvxpu6A', 'publish_date': '2024-01-24 00:00:00', 'length': 1272, 'author': 'LangChain', 'publish_year': 2024}, page_content=\"streaming is uh an incredibly important ux consideration for building L Ms in a few ways first of all even if you're just working with a single llm call it can often take a while and you might want to stream individual tokens to the user so they can see what's happening as the llm responds second of all a lot of the things that we build in the laying chain are more complicated chains or agents and so being able to stream the intermediate steps what tool are being called what the input to those tools are what the output to those tools are um streaming those things is really important for a nice ux as well so we've recently added a new method in Lang chain to help with this called stream events um which hooks into the Callback handlers we have so it's very easily modifiable and what it will do is it makes it easy to stream everything basically and so this video will cover uh stream events so we'll also cover the basic stream and a stream methods that we have so these are simpler methods for dealing with the end responses of of chains or of models and it will take a look at doing this for generic rentables and then for uh link chain agents as well so let's jump into it this is the docs right here little known fact all of these docs are also notebooks so I'm going to open up the notebooks version of this um let's restart my kernel um just so I can do it with a clean end um and yeah we're going to cover two main things here we're going to cover the stream and then a stream methods and these are methods for streaming the final outputs of chains and then we're going to cover the stream events um method we're going to cover the stream events method and this is a way to stream all the events that happen within a chain so um all runnable objects in link chain and so these are every these are all chains these are all models these are basically all objects in link chain EX Expos a sync method called stream and an async variant called a stream and so these methods will stream the final output in chunks yielding each chunk as soon as it's available and so as we see as we'll go along streaming is only available if for if every step in the program kind of like knows how to stream things along so if you're working with one step in the program like an llm if it knows how to stream then you can use stream if you're working with multiple maybe one of them doesn't know how to stream and we'll see an example of this later on then it might not stream so let's start by looking at the llms themselves so we'll use an anthropic model here and basically what we'll do is we'll iterate over the chunks in uh model.am things that come out of these are going to be AI messages um and so uh what we're going to do is we're going to append them to a list and then we're going to print um chunk got do content um and so this is just a Content field on the aage and we're going to print this little operator to to show this the spaces between the streams so we can see that it starts to stream as it responds if we look at one of these chunks we can see as AI message these AI messages we've we've made kind of like addable under the hood so you can add them up and you can get the final response okay so that's a simple llm now we're going to look at chains and so in this chain and so chain is kind of like anything constructed in L chain expression language this chain will just be a really simple one where we have a prompt a parser um and then the chain is prompt model parser um we will invoke it uh with a stream so we'll use async here and we'll stream over things and we can see what happens as it goes through and so basically we're asking we have a prompt that's formatting it with uh uh it's tell me a joke about blah um we have this model and then we have this parser and this parser is just responsible for taking the cont content property off of the message and so it just return to string instead of this AI message the model and the pars Expos stream method so we'll see that it will stream out as it completes there we go all right so we can also see an example of other things um so uh a more complicated parer that we have um is the Json output parser which takes in a response from a language model that is supposed to be Json um but it might be like half completed Json right so this happens when you're streaming the the like when you when you're halfway through the stream of a Json blob it's not valid Json and so this parser deals with a lot of those peculiarities and parses it accordingly and so we can see here that it will start to stream out the more completed kind of like Json blob as we create it here we'll see an example um where this doesn't stream um so this is a method that just returns all the country names um and so here if we add this on the end of The Blob we can see that there's actually no streaming that will happen because this final thing does it it doesn't have it doesn't yield things it doesn't stream it doesn't take in it doesn't take in a a generator and it doesn't return a generator and so it kind of blocks everything so it just returns it once at the end if we want to change that um we can change this and this will start to yield things and so why is this happening um I think this is happening because I have a uh I have some flush issues here um let's do this see if this changes things yeah so we can see here once I add in this so there's some flush issues going on but basically we can see that it streams back um tokens and again that's because this yields things com some built-in components like retrievers these don't stream anything so if I call uh the um if I call this it just returns a list of documents we haven't really seen a need for retrievers to be streaming so that's why we kind of haven't implemented a version of that but you can still use these within chains um that uh uh you can still use retrievers within chains and still stream the final results so here's like a really simple rag chain that gets the context back um so basically calls a retriever gets some documents passes it to a prompt passes it to a model passes it to an output pars here these still stream so we'll see that the final response we can stream all right so those are covering kind of like the basics of stream and aam the the big uh kind of like limitation of these is that it only streams the final kind of like output of a of a chain often times we might want to stream the the intermediate steps or the events that happen within the chain along the way so we'll walk through this new method um stream events um and the events that it produces are aligned with the Callback handlers that we have and so we have a bunch of different types here that we've listed here um so we kind of like have uh uh so this is for chat models and llms very similar we we yield something on the start we we yield something on the end and then on stream we also yield each token if the model support streaming same for llms um same for chains same for tools same for retrievers and then same for prompts prompts just has start and end um one thing to note is that this is a beta API um and would love to hear any feedback about it so please let us know so here we can start we can use the the we can just use the um model mod and we can we can call a simple model and we can take a look at the stream events that are associated with this model one thing that you may notice is this version thing in a stream events so what is this uh basically we're documenting kind of like the API for this a stream events um interface as mentioned it's a beta interface it may change so we want to uh make it make it easier to do that in the future let's take a look at what some of these events are so we can see that the first one is on chat Model start so it starts kind of like the the streaming the first event it hits the chat Model start and then it's chat model stream chat model Stream So basically what's happening is the input's going in it's starting the chat model and then after that it's getting token by token if we look at the last results we can see that it's on chat model stream and then on chat model end so this is a very simple single call to a chat model that first calls the event on chat Model start then calls stream on each token and then calls on chat model at the end let's take a look at the chain that we have so now we have this model and we have this Json output puror let's gather the list of events and we can explore them this way so the first three events that we see are actually all on blank start so first we have on chain start because we enter the overall chain here then we have on chat Model start because that's the first one but then the model streams things through so it passes a generator through it passes that generator through right away to the output parser and so we immediately hit kind of like on parser start we can explore this a little bit more by looking at the by looking at some different events so let's look at maybe like the last three events in here we can see that we have on chat model end on parser end and then on chain end um so this is this this makes sense it's uh you know if we imagine kind of like the the SE quence of of chain encapsulating a model and then a parser basically the model will kind of like finish then the parser will finish and then the chain will finish let's look at maybe some of the middle ones so if we look at like 3 to six we can see that what happens maybe let's do 3 to seven for just a slightly nicer thing um we can see that it's a mingling of of on chat model stream on Pur stream on chat model stream um sorry on chat model stream on par of stream on chain stream so what's happening is as the chunks occur because we stream them all the way through we first see one token come out of the chat model that gets passed through to the parser that gets passed through to the end of the chain and so it's the sequence of events so now we can take a look at uh using these real time to do some streaming so we will do this by parsing the revent the the the events that get emitted by the stream events so first we'll take a look at each event and we'll take a look at what the kind of the event is then we can basically do things depending on what the kind is and we'll just do some simple string matching based on the name so if the chat if it's on chat model stream this is when it's printing out a token we'll P we'll print this chat model chunk and then if it's on parser stream this is when it's uh going through the output parser we will pass that and we'll set flush equals true this will get rid of some of the flushing issues that I had above and yeah we'll we'll just only do this for the first 30 so we can see now that we start printing things out and it's intermingled with chap model and parser because basically the chat model starts accumulating tokens and at some point then the parser emits something um but but they start streaming kind of like at the same time there might often be a lot of different events coming out from your chains when you have complicated ones with more types of models um um different types of tools subchains things like that and so we want to make it really easy to basically um filter these events we have a few ways of doing that one is by name so you can assign any runnable so again a runnable is just a model or a prompt or a chain or an output parser you can assign it with a run name and then you can filter um the events by include names so here we give uh the model the Run name model the output parser the Run name my parser we have include names only my parser and then if we print out events we will only get events related to the output parser because we're filtering out the ones with the Run name model if we switch it up and we go model then we only get ones associated with the model another way to filter is by type so we have the same chain here um but we're just going to uh change include types and so now include types we're going to set next to chat model this will get all chat models so if you want to get all chat models regardless of the name that you give them um you can you can get them this way tags are another way to do it tags are basically inherited from any child component so here we're going to give this whole chain the tags my chain and then we're going to stream all events and include tags my chain my chain with so the way that tags work it's not just events associated with this chain it's with any sub child components as well so if we stream it out we see that we start to get everything we get chain um we get onchain start on chat model on parser start in this case because we taged the whole chain with my chain this is everything if we take a look at non-streaming components again we can see that so this is the this is the one that blocks the final result um doesn't stream if we look at a stream again we this is the same example as above it doesn't stream but if we do stream events it does and because that's get they get passed through up until that point so we can see kind of like everything going on another thing this is a slightly more advanced one is basically if you have um if you have basically custom tools and they have inside them a runnable um then what you want to do is you want to propagate the call back so that otherwise no stream events will be generated a common example of this is if you have a tool for an agent and that tool calls an llm you need to make sure to propagate the the callbacks correctly um so here is an example of it not propagating correctly um and so you can see that there is basically just the stuff for tools because even though it calls this runnable inside of the tool the callbacks aren't propagated so it doesn't actually know that exists if we now propagate them and you can do that by just adding in callbacks as an argument to this tool function um and then passing it in through callbacks here um you can see now that we get this onchain start and onchain end event and these occur in the middle and so these are I think more examples of just the same all right so that covers basic streaming stuff we get most of the questions around streaming around agents and so I want to show two examples of doing that so this is the streaming page in the Lang chain documentation that shows how to stream with the agent executor so let's restart the kernel so we can see what's going on here if you haven't if you don't know what agents are check them out in a separate video I'll link to one in the description basically with agents the first thing we're going to do is we're going to define a model and we're going to make sure that streaming set equal to true this is so that it's this is necessary so that it it streams no matter where it's called from so in an agent it will be called within the agent many times so we set streaming equals to True um and then we Define our tools um and so these are just two example tools tools um we can play around with them and then we initialize the agent and so here we can see that we are going to first pull a prompt and we're going to use this prompt for agent we're then going to list out the tools we're then going to create the open AA tools agent and we're going to give the model here a tag agent llm and then we're going to create the agent executor this is the runtime for the agent um and we're going to give it run name agent this covers streaming so this is the streaming of the agent the streaming of the agent does the individual steps so that's nice but often times we want to get the individual tokens as well so let's stream let's go down to custom streaming with events so here we can do the same thing that we had before so we have the input we'll use version one we're using a stream events and now we can start doing stuff with the uh the events that are emitted so if onchain start and if the event name was agent so this is basically this is if you remember back up here we tagged the agent executor with the Run name agent so this is basically saying whenever on on the start and end of this we are going to print out this and then same thing on the end of this we're going to print out this and this is needed because there's a lot of subchains within this agent um there's places where we kind of like do the prompt into the model um those are subchains we really care about this overall agent and then we care about the streaming of the tokens from the chat model so we are going to uh basically if it's on chat model stream and if the content is exists and so there's actually cases where the content doesn't exist and this is when tools may be called we can we can play around with this and see what that looks like after but for now only if there's content we're going to stream that and then we'll also stream the tool start and Tool end so if we do this we can see that it starts printing things out you can see that it really quickly streamed that um and then gets that so if we take a closer look we can see that it prints out starting agent we then get a bunch of information about the tool um tool one tool two and then it starts uh streaming the final response we can also change this so that it always prints out the content and so now what we'll see yeah so we'll see here so these things are when it actually so okay so this is when the content didn't exist we can maybe even just print out what exactly it was yeah here we can see that it prints out the whole chunk and so we can see that the tool calls um and it's still streaming kind of like stuff um which is why it looks incomplete but basically this is printing out the um individual chunks which have the chunks of the tool call in it so there's no content but there's the chunks of the tool call and that's what we're printing out here that covers streaming with agent executor there's one more thing I want to go over which is streaming with L graph L graph is a package that we introduced that makes it really easy to create uh agent run times as graphs and or state machines essentially the same thing and so because Lan graph is built on top of Lane chain and is uh runnable at the end of the day it has this exact same interface so if we go over to L graph we can see in example of this um let's also restart the kernel here we can create our tools we can create our tool executor which just runs tools we can create the model again we set streaming equals to True um for for chat open AI um we bind the functions to the model so it knows which tool it has available we're now creating the nodes in lingraph if you aren't familiar with this I'll also link to a video for lra so you should check that out we just did a whole series of them on on YouTube We Define a bunch of our nodes here uh um again we'll cover this in a video separately or it's already been covered in a separate video we Define our graph here um and then this app that we get back this is a runnable like any other thing that's constructed with Lang chain expression language so we can start taking a look at it and uh uh using it in the same way so we'll use a stream events we'll we'll look at the type of the event if it's a chat model Stream So if it is a uh token basically we'll do the same thing where we print it out if it's not empty um and then we also print out the on tool start and the on tool end so we'll we'll run this with the inputs what is the weather and SF we can see that we get back or it's logged the the starting input for the tool we get the output and it start streaming back the response um of the tokens there that's basically all I wanted to cover in this video streaming is super important to llm applications and so hopefully this new stream event method will make it really easy to stream back what is going on inside your applications to the end user again this is in beta so please let us know if you have any feedback or questions thank you\")]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'HAn9vnJy6S4',\n",
       " 'title': 'OpenGPTs',\n",
       " 'description': 'Unknown',\n",
       " 'view_count': 9190,\n",
       " 'thumbnail_url': 'https://i.ytimg.com/vi/HAn9vnJy6S4/hq720.jpg',\n",
       " 'publish_date': '2024-01-31 00:00:00',\n",
       " 'length': 1530,\n",
       " 'author': 'LangChain',\n",
       " 'publish_year': 2024}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checkig the metadata\n",
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OpenGPTs',\n",
       " 'Building a web RAG chatbot: using LangChain, Exa (prev. Metaphor), LangSmith, and Hosted Langserve',\n",
       " 'Streaming Events: Introducing a new `stream_events` method']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the title of the documents loaded\n",
    "[doc.metadata['title'] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hello today I want to talk about open gpts open gpts is a project that we built here at linkchain uh that replicates the GPT store in a few ways so it creates uh end user-facing friendly interface to create different Bots and these Bots can have access to different tools and they can uh be given files to retrieve things over and basically it's a way to create a variety of bots and expose the configuration of these Bots to end users it's all open source um it can be used with open AI it can be used with other models as as we'll see um and it's an exciting way to create a a GPT store like experience if you're building a more focused platform an internal platform or any of that so we launched this a few months ago actually right when uh open AI released their GPT store and but we haven't really dove into what's going on or how to use it um and so there's several things that I want to cover in this video there's maybe two main areas one I want to talk about it as an enduser facing application so how can you interact with we we have a we have a simple research preview hosted version of this um how can you interact what can you do what the functionality in the second half of the video I want to talk about how openg gpts is built um we do not intend to uh you know monetize openg gpts we want this to be a platform that people can clone and spin up their own versions of the GPD store or an internal platform or things like that so the second half of this video will focus more on how to build this platform and some of the considerations that we made please note that I am filming this on January 30th we are going to continue working on this continue changing it so if you are watching it later the hosted version or the code may have changed so starting with how to use it um the the the first thing that you'll want to do is create a bot and we actually have three different types of bots that one can create I'm going to start with assistant which is the default bot and and is also the most powerful one so the assistant can use an arbitrary number of tools and you can give it arbitrary instructions the llm will then take those instructions and those tools and decide which tools if any to call um get back the response and then and then continue on its way so let's create one that has access to the internet um so let's name it internet bot um let's give it some instructions you are a helpful weatherman always tell a joke when giving the weather let's scroll down there's a bunch of tools that we can use I'm going to use the search by tavil um they're a LM focused search engine um and then the currently the only agent type that we have supported in the research preview is with GPT 3.5 turbo but we will show in the code when you're building this you can enable CLA you can enable uh Google Gemini you can use gp4 you can use Azure you can use Bedrock but this is just the one that we have in the research preview so I'm going to save uh this now I'm going to try chatting with it let's say hi so several things to notice um one it's streaming responses so we put a lot of work into streaming of tokens this is important for uh uh you know most chat-based applications um it's got some feedback as well so I can give it thumbs up thumbs down um and start recording feedback and and when we talk about the platform side of things we'll see how that can be used now let's ask it a question that will require the search ability what is the weather in SF currently we can see that it decides to use a tool and so importantly it lets us know that it's deciding to use a tool and then it also lets us know what the result of the tool is and then it starts streaming back the response so this is streaming not just tokens but also these intermediate steps which provide really good visibility into what is going on we can see here we can see the response that we got back from tavil um and then we can see um the response from the AI and so there's lots of dad jokes in here this is using open aai tool calling under the hood so we can also ask it to look up multiple things and see multiple tool calls in parallel what about in LA and in New York so we can see that it now calls two things two things it C it call calls the tavil search Json for weather in Los Angeles the tavil search for weather in New York we can look at the responses this is for Los Angeles this is for New York um and then we can uh see the response here so this is an example of parallel tool calling that's enabled with open ai's most recent feature their their tool calling so this is the assistant it's using Tools in an arbitrary way to accomplish its task let's uh go create a new bot and now let's create one uh of type of rag so rag is really focused on retrieval over arbitrary files that you can upload so you can upload files and then the uh you can also give it custom instructions um and and then the bot will respond based on those files what is the difference between this and the assistant because in the assistant you could also upload files and you could choose retrieval as one of many to tools the main difference is that this is much more focused on answering questions specifically about files that you upload so this means that it will always look things up in a retriever it's actually hardcoded and we'll show this in in the um when we talk about the the back end but it's actually hardcoded that it will always do a retrieval step here the assistant decides whether to do a retrieval step or not sometimes this is good sometimes this is bad sometimes it you don't need to do a retrieval step when I said hi it didn't need to call it tool um but other times you know the the llm might mess up and not realize that it needs to do a retrieval step and so the rag bot will always do a retrieval step so it's more focused there because this is also a simpler architecture so it's always doing a retrieval Step at the start and then it's actually always responding after that it's not doing potentially two retrieval steps it's not doing an iterative search this is a very simple rag architecture um which has its downsides it's not as flexible it can't handle multihop questions things like that but it's much more focused streamlined and that means it can work with simpler models as well so we have actually enabled mraw um through fireworks to work on uh this type of gbt a rag gbt so I have a uh I have a PDF here that I'm going to upload um this is Spade this is a paper Shrea recently wrote um at Berkeley and it goes over setting up kind of like a testing pipeline um Bas for for your prompts super interesting paper um I'd uh I'd recommend reading it regardless um let's upload the PDF um we can we can change the message slightly um let's still use 235 turbo research Spade let's save this now it's taking a little bit longer to save because what's going on under the hood is that it's injesting the file now it's in a method where it can be retrieves and and I'll talk about this when we talk about the back end as well um if let's take a look at this paper um and let's figure out something we can um ask what is a propped Delta so here it always uses retrieval the the it calls the retrieval function it gets back documents we format documents nicely um and you can see what these documents are and then it responds here so this is an example of a simple rag bot which always does retrieval hyperfocused on rag if you want to ground a bot in some external data source that you can upload this is probably the simplest and most reliable way to do that again it's a little bit less trustworthy than uh than than the chat bot we have it as a separate type of Bot because it is simpler so that means that it can work with other models like mraw which is an open source model so it just provides more flexibility and that flexibility is the same reason we have a third type of Bot this chat bot this is just solely parameterized by the instructions so you can write out long complicated instructions for how it should behave you can give it a character and it can act like that again because this is simpler it can work with simpler models let's create an example chatbot we'll create one that responds like a pirate so you are a helpful pirate always respond in a pirate tone pirate save this hi and we get back a response and pirate so a lot of gpts in the GPT store are really just complicated system prompts and so for those you can create them using using this chatbot type the other a lot of the other gbts that I've seen at least are the rag style chat Bots where they're parameterized by a system prompt and then also um and then also a bunch of uh files that you can upload to give it information besides what it knows about and it can search over those and so these you know these are much simpler architectures than the assistant but for a majority of use cases they're actually completely fine the nice thing about assistance is you can do more complicated with things things with it and you can also equip it with arbitrary tools and so here there's a bunch of tools that we've enabled in the back end by default but you can easily add your own and and explore with those um and so that's part of the power of this platform being open source as well you can Fork it you can make it your own you can deploy it either to end users internal company users anything like that other things that I want to highlight in the front end um you can make Bots public this means that you can share links um you can see old conversations and jump back in um you can create new chats um when you're in a conversation you can click in here and see the bot that it is using um when you create a new chat you can look at the saved Bots that you have if you have any public Bots they'll be down here as well and yeah that's basically it for an overview of the front end I'm now going to switch to talk about some of the architecture of the back end which will be really helpful if you want to Fork this and make it your own so this is the openg gpts repo it's under the linkchain org um there's some instructions here there's a good read me on everything that's in here there's some Docker composed files for deploying it um there's some other uh uh files for environment variables the API docs things like that for this I really want to focus on the back end so we can take a look at what's going going on in here um and most of the logic here there's there's some requirements files most of the logic here is going to be in app and so we can see there's a bunch of different files here so there's a few things that I want to draw attention to first let's uh let's maybe look at agent types so when we talk about the assistant in the in the uh in the front end this is where these agent types are defined and so different assistants have different architectures that are going on behind the hood let's take a look at the open AI agent for example so important to note this is built on top of L graph so if you aren't familiar with L graph you should definitely go check it out it's a really easy way to build these types of cyclical agentic Frameworks so we have this open AI agent executor which takes in a list of tools an llm a system message and then a checkpoint um and we'll see how we use this so first um we're going to create basically this uh quote unquote agent and this agent is responsible for taking in messages and deciding what to do next so there's first a step where we format um the messages um and so we add a system message um and that's defined up here and then we pass in the rest of the messages the llm also then has access to the tools so we bind it with tools and then we combine it using this pipe syntax to get this agent we next Define a tool executor this is a class that is just does some minor boiler plate for calling tools um and then we start to define the different nodes of the graph so first we Define the function that determines whether to continue or not this is should continue it looks at the messages if there's no tool calls then it finishes if there is tool calls then it will continue and we'll see how we'll use this later on We Now define the node that calls the tools um so here uh we uh take in the list of messages if we we get the last messages we get the last message we know that it involves a function call um because otherwise we would have ended um we get all the tool calls so when there's multiple tool calls we get them all we then pass them in uh into here into the tool executor in a batch so it runs them in parallel um and then we append them to the the messages and we return the messages from this node so importantly this will use a message graph and so this means that every node in the graph should return a message or a list of message so here we return a list of messages the other node that we add the agent this is just this is basically an llm call the LM call returns a message so both of the nodes return messages we set the entry point to the agent so when anything comes in we go to the agent we then add a conditional Edge so after the agent is called we then check this should continue function if uh it says continue then we call the action node otherwise we finish um we add an edge so after we call the tools in the action node we go back to the agent we then compile it compiling we're passing in check pointer equals checkpoint this is basically a way to persist um the state of the graph so we're persisting all the messages that that happen so this is nice for a few reasons um the main immediate way that we are using it right now is we are saving it to reddis and then we're showing that in the front end so in the front end when you see that we save the chat history um those are pulling from reddis the way that is getting saved to redus we don't have separate functions saving everything we just pass in this checkpoint and it all kind of gets written there there's similar things for the Google agent Google agent looks very similar there's some minor differences uh because it uh uh is a a Google agent so it's a little bit different it doesn't have tool calling it has function calling there's also an XML agent designed to work with anthropic models and so this is different as well so it uses some of the prompts um and things like that so those are the agent types um we also have a uh really simple executor for the chatbot chatbot just calls the message once with the system message so it has a really simple node the chatbot node just calls it and then ends dead simple um but we use the message graph again uh so that it it all of these Bots can speak kind of like on the same kind of like State um which will make it nice if we want to do any multi-agent or multibot things in the future um and then we also have this retrieval bot so this retrieval bot um basically it it's it had it's simpler than the agent node um so it doesn't have any Loops but it's more complex than the chatbot node so we have this prompt template um this is used for coming up with a search query to pass to the retrieval um and so we can see that we have the conversation here and then we generate the search query um and then the response pop template takes in instructions and then has context so what's going on here is that we have this get messages function um and basically what's going to happen is we're passing all the state around as messages um and so part of that um has the the search query involved and so we can see here if we scroll down I'm going to scroll down to this graph we F we have this invoke retrieval uh node we have this retrieve node and then we have this response node and then we always invoke retrieval at the start and then we go from invoke retrieval to retrieve and then we go from retrieve to response and then we end so remember how I said the difference one of the differences with the retrieval with the rag bot was that it always did retrieval this is this is what's happening so the invoke retrieval node it's always going to return an AI message that calls retrieval so we're not even actually calling the language model sometimes so if the length of the messages is one this means if it's the first message in the conversation we're just going to look up whatever that first thing was um so this is a little cheat that a lot of rag based systems or conversational rag Bas systems do is the first time someone types in something in we just look up that input the issue start starts to happen when you have a conversation so if I have a follow-up question or you know a series of follow-up questions I don't really just want to pass that follow-up question in because it could be referencing things previously and so what I do instead is I call um this other method um which is itself a call to a language model um and so this is using the search prompt to generate a search query um and then I specify that as the the retrieval thing then the retrieval thing is just calling the retrieval it's passing the results in this function message um and then the response is just a call to the language model with some formatting into this prompt so if we see this get messages thing what we're doing is we're getting the uh most recent message which is the result of calling this tool we know that it will always be that because we have this this determined graph we're getting the response from that and we're we're formatting that into the system message um and yeah so basically Al we're constructing the chat history the chat history is going to be AI messages that do not have function calls if they do have function calls then they are the result of retrieval steps and we don't want to include those in the messages that we pass to the final LM um and then uh the chat history also includes human messages um and then it also includes the system message system message is where we use the system message prompt as well as the context that we retrieve from the documents so that is the so those are the three types of um Bots that we have they're all put together in this agent file and this is where we also start to use um uh uh configuration basically so configurable fields and configurable alternatives are something that exists in linkchain and they're really handy when you want end users to be able to configure things or sometimes when you want to do the configuration on the Fly for example if you want to randomly select a model to use and you want to configure that on runtime and basically the way that that looks like and the way that we've implemented it here um is that we have this idea of like a configurable agent that wraps around a binding and there's these different parameters on here inside the initialization we take in the parameters and we construct the agent um and then we pass it in and then what we'll do down below and I'll return to this later on what we do about down below is we initialize this configurable agent and then we Mark certain Fields as configurable so agent field here is configurable with an agent type um the system message field is configurable with a system message um the assistant ID this is the assistant ID of the um of the bot that you've selected the tools are configurable the retrieval descriptions configurable and so a lot of the uh fields that we create the bot with are configurable and those are exactly what we expose in the front end of uh open gpts we've also exposed some configurable Alternatives and these are the different architectures so uh there's um there's a chatbot which follows the chatbot architecture and there's the chat retrieval which follows the rag bot architecture and if we scroll up we can see that chat retrieval uses this idea of configurable retrieval um so this is the same kind of like runable binding it's got these same parameters we Mark these fields as configurable um and then chatbots exactly the same and so basically the difference between the fields these are things that go in again we have like three seate types of high Lev Bots the configurable fields are things to configure this specific the the assistant type of Bot and then the alternatives are completely different kind of like Alternatives that you could even use so there's two different ways that you can configure things fields and Alternatives one more thing I want to highlight is just the ingestion bit um so there's the ingest pipeline um which is really quite simple this is something we're going to look to expand on in the future um and so if if you want to help make this uh retrieval more advanced would love that but basically we just split documents um and then we add them to a simple Vector store um and then the retrieval um is here part of tools I believe um and we can see here that it's just a uh retriever um really simple um really simple and then we just filter based on the name space so uh based on the assistant that you're using the assistant only has access to the files that were uploaded to it um so there's a lot that we can do to improve this we can add a reranking step um there's already some sort of query transformation going on based on the Bots um but we can add more things um and so improving the retrieval is one aspect that we want to lean into in the future um the last thing I want to point out is that this all integrates with Lang Smith so if you're a little bit lost about what the different types of agents are what exactly is happening uh uh what got configured um that's totally normal these applications start to get really really complex and that's where lsmith comes in handy so the deployed version we have hooked up to a project in lsmith and so we can click on here we have this open gpts example project um and so if I go in here and I click on a trace I can see exactly what's kind of like going on under the hood um and so here um this is if you remember this is actually the system message that I added um when I configured the pirate chatbot um and so this is a system message this is what I said and is the response and so I can track it and so I can also leave when I when I left the thumbs up and thumbs down um I can track that here um and so I believe yeah so here I left the thumbs up on this was this was the weatherman um I I got back this response if we click in what's here I think yeah so this is when we gave it access to the Search tool um and so you can see like exactly what's going on um this is a pretty simple agent because it just responded um but I can see the feedback here as well it's at the top level so I can see the feedback here as well I have a user score of one um and so yeah Lang Smith is a whole separate concept but I just want to point out that openg gpts if you deploy it it's integrated with Lang Smith um if you need Lang Smith access shoot me a DM on Twitter or LinkedIn and can get you access to that that's pretty much it for what I wanted to cover um hopefully this gives you a good sense of both how to use the front end the the research example as well as how to configure the back end um so I think I think the important thing to note that I would highlight is that you can put any different type of architecture behind here right now all the architectures the three different architectures we have the assistant architecture the rag architecture and the chatbot architecture they all use this message graph implementation which passes around a list of messages and I really like this because it's a single common uh kind of like interface that'll make it easy to add on different bots in the future um so there's a lot of things we want to do in the future one of those is having like multiple bots on the same thread or allowing you to switch Bots between threads um and so having this common state representation will make it easy to do that that's pretty much all I got hope you guys enjoyed this we're really excited about open gpts if you want to use this I mean one feel free to Fork it but also feel free to reach out to us we're more than happy to help thanks\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the content of first document\n",
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda\\envs\\venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "c:\\Anaconda\\envs\\venv\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Anaconda\\envs\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "splitter =  RecursiveCharacterTextSplitter(chunk_size=300)\n",
    "split_docs = splitter.split_documents(docs)\n",
    "db = FAISS.from_documents(split_docs, HuggingFaceEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class Search(BaseModel):\n",
    "    \"\"\"Search over a database of tutorial videos about a software library.\"\"\"\n",
    "\n",
    "    query: str = Field(\n",
    "        ...,\n",
    "        description=\"Similarity search query applied to video transcripts.\",\n",
    "    )\n",
    "    publish_year: Optional[int] = Field(None, description=\"Year video was published\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python-dotenv could not parse statement starting at line 1\n",
      "Python-dotenv could not parse statement starting at line 3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['GROQ_API_KEY'] = os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "system = \"\"\"You are an expert at converting user questions into database queries. \\\n",
    "You have access to a database of tutorial videos about a software library for building LLM-powered applications. \\\n",
    "Given a question, return a list of database queries optimized to retrieve the most relevant results.\n",
    "\n",
    "If there are acronyms or words you are not familiar with, do not try to rephrase them.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "llm = ChatGroq(model='llama3-8b-8192')\n",
    "structured_llm = llm.with_structured_output(Search)\n",
    "query_analyzer = {\"question\": RunnablePassthrough()} | prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Search(query='build a RAG agent', publish_year=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_analyzer.invoke(\"how do I build a RAG agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Search(query='RAG', publish_year=2023)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_analyzer.invoke(\"videos on RAG published in 2023\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval with query analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval(search: Search) -> List[Document]:\n",
    "    if search.publish_year is not None:\n",
    "        # This is syntax specific to Chroma,\n",
    "        # the vector database we are using.\n",
    "        _filter = {\"publish_year\": {\"$eq\": search.publish_year}}\n",
    "    else:\n",
    "        _filter = None\n",
    "    return db.similarity_search(search.query, filter=_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain = query_analyzer | retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = retrieval_chain.invoke(\"web rag chatbot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('OpenGPTs', '2024-01-31 00:00:00'),\n",
       " ('OpenGPTs', '2024-01-31 00:00:00'),\n",
       " ('OpenGPTs', '2024-01-31 00:00:00'),\n",
       " ('OpenGPTs', '2024-01-31 00:00:00')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(doc.metadata[\"title\"], doc.metadata[\"publish_date\"]) for doc in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
